{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Attention Is All You Need) 구현하기 (1/3)\n",
    "- [code 참고 : transformer 구현하기(1/3)](https://paul-hyun.github.io/transformer-01/)\n",
    "- [이론참고 : Attention is all you need 뽀개기](https://pozalabs.github.io/transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/mac/opt/anaconda3/lib/python3.7/site-packages (0.1.95)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vocab\n",
    "- sentencepiece로 미리 생성해둔 vocab을 이용해 입력 텍스트를 tensor로 변경\n",
    "- line 21: batch_first = True할 경우 default인 BxT -> TxB로 나와 보기 편함\n",
    "    - B : batch_size\n",
    "    - T : length of the longest sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁겨울', '은', '▁추', '워', '요', '.']\n",
      "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8]),\n",
       " tensor([[3159, 3533,  200, 3883, 3688, 3519,    0,    0],\n",
       "         [ 206, 3534,   53, 3759, 3525, 3613, 3688, 3519]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab_file = \"./data/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "\n",
    "# input texts\n",
    "lines = [\n",
    "    \"겨울은 추워요.\",\n",
    "    \"감기 조심하세요.\"\n",
    "]\n",
    "\n",
    "# text to tensor\n",
    "inputs = []\n",
    "for line in lines:\n",
    "    pieces = vocab.encode_as_pieces(line)\n",
    "    ids = vocab.encode_as_ids(line)\n",
    "    inputs.append(torch.tensor(ids))\n",
    "    print(pieces)\n",
    "    \n",
    "    \n",
    "# 입력 길이가 다르므로 입력 최대 길이에 맞춰 padding(0)을 추가\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first = True, padding_value = 0)\n",
    "# shape and value\n",
    "inputs.size(), inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embedding\n",
    "- Transformer의 임베딩은 아래의 두 가지 방법을 합해서 사용\n",
    "1. Input embedding\n",
    "2. Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Input Embedding\n",
    "- 임베딩은 입력 토큰을 벡터 형태로 변환함\n",
    "### 1) 인풋에 대한 임베딩 값 input_embs구하기\n",
    "- inputs(2,8)에 대한 임베딩 값 input_embs(2,8,128) shape을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = len(vocab) # vocab count\n",
    "d_hidn = 128 # hidden size\n",
    "nn_emb = nn.Embedding(n_vocab, d_hidn) # embedding object\n",
    "\n",
    "input_embs = nn_emb(inputs) # input embedding\n",
    "input_embs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Position Embedding\n",
    "1. 각 포지션 별로 angle 값 구하기\n",
    "2. 구해진 angle 중 짝수 idx 값에 대한 sin값 구하기\n",
    "3. 구해진 angle 중 홀수 idx 값에 대한 cos값 구하기\n",
    "\n",
    "**position encoding function**\n",
    "- [계산 참고](https://skyjwoo.tistory.com/entry/positional-encoding%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80)\n",
    "- [positional encoding table 이해 참고](https://www.youtube.com/watch?v=dichIcUZfOw&t=639s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sinusoid position embedding\"\"\"\n",
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5gcZZn38e9vDjmSA+RESAIBDEeBgAgIrIKAhiwC8uoaVhAPbFBB8bQalnXdXXX11fUsGEdEWFdBBIIRIwd50agIJkCAhICEgCEkEMIhRE6TmbnfP6pm6OlMZrozXdM91b/PddU1/VQ99dTd3dN3Vz9V9ZQiAjMzy4+GagdgZmaV5cRuZpYzTuxmZjnjxG5mljNO7GZmOePEbmaWM5kmdkkfl7RC0nJJV0galuX2zMyqQdKlkjZIWr6N5ZL0bUmrJN0r6ZCCZbMkPZgum1eJeDJL7JKmAB8FDo2I1wKNwJystmdmVkWXAbN6WX4iMCOd5gLfA5DUCFyULt8POF3Sfv0NJuuumCZguKQmYASwLuPtmZkNuIhYDDzTS5VTgP+JxO3AWEmTgcOAVRGxOiJagSvTuv3S1N8GtiUiHpf038Aa4CXgpoi4qbiepLkk32A0DBn2uiFjpwCw/4RGAO565FkApr3yfNc6L+62R7Lu6ocB2PWAPQH428Nr0kaTPyOn7QzA2gceA2D8gckX4VP33g9A2/SknTGPPdrV9roddwFgRsfTAKxsTXqPZu48FIC7H38BgIP3Surd/dD6JN4Z0wC4/+Hku2v6rpMAWLPu1fd63PjRADz73MsADB3enMSxpb3ztUj+Nqjb/GFpvRf/lqw3dsyIpJ1nN3e1PTFt+8kNzwEwZfJOADz++FPJazRtIgB/XfMkAHvslsS3+tEnAHjN7pMBeOiR9V1t7rNH8hwfePhxAPZ9TfLerFy1FoD9ZkxNnvNDa7u9BiseSl7v16bl5X9J3pfX7rVrV9ud8w7YO5l334M9lw9My/d2lvdJyw/0XAY4KJ13Tzqvr/LMtLysj3LvdXZLy3/drjLAzH3TeSv/WpFyJdo4OH2ed6fP8+CC16J4XqXKpdSJl57eGBET6IeG0VODtpdLqhsvPb0CKKzcEhEtZWxuCvBYQXltOq+n+YeX0W6PlNWQApJ2BK4B3gU8B/wcuDoi/ndb6+wwde+Y9s6vArD8n5JEtcP7fg7At/5yS1e9pd+/EoDmOacC8O3V1wHwx3d9ONl2mhiP+OY/A/Dpoz4OwNmP3QPAxVMOBGDjpVcBcNKnzupq+99O+zwAv3r58qSNNcmXwYZPTwdg9IVLAHjhpn9P4puV1H/gxq8DcOBp/wHAZRd9AoAP/duVXW2/9/1vBeDnv1wJwJ4HJl88T61NvrSahyZfZsNGDEniW5fM3+eAJAnf9YeHADj5bQcBcM2Vi7va/tDcE5LX4tsLAPjiv54OwGcu/AEA87/2IQDmnv9tAH42P4nvHWd/GYBFl18IwIln/GdXm7+/Knl89DuSZX++7r8AeP3JnwHgnl99JXnOJ34SgBU3JK/B/rOStv9y0zcAmHH8RwFYfcu3u9re47hk3l9v/Q4Aux37EQDW/DYp73pMUn78d98FYMqbzgNg/eKkPPmNSfmJtLxzWgbY8Ptk3sS/S+Y99YeLAJhw9Lk9lp/+Y1Ied1RSfiYt71RU7mleZ/m52y4GYOyRH96uMsCmPyXzxrzhwxUpV6KN529LnufoI8/tVu5pXqXKpdTZsuxHd0bEofRDw4jx0bT3ySXVLWV7kqYD16ddz8XLfgV8KSL+kJZvAT4N7AG8NSLOTuefCRwWER8p46lsJcuumOOBRyLiqYjYAlwLHJnh9szMSiehhsaSpgpYC0wrKE8l6Zre1vx+yTKxrwGOkDRCSR/DccDKDLdnZlYG0dA0pKSpAhYC70nPjjkC2BQR64ElwAxJu0saQnKCycL+bizLPvY7JF0N3AW0AXcD5fRJmZllJ91jr0xTugI4BhgvaS3wOaAZICLmA4uA2cAq4EXgfemyNknnATeSnDl4aUSs6G88mSV2gIj4HMkTNDOrKQLUWJnEHhGn97E8gHO3sWwRSeKvmEwTu5lZzZJoqNAee61xYjezulWprpha48RuZvWpgn3stcaJ3czqkhANTc3VDiMTTuxmVp+8x25mlj9O7GZmeSJV7HTHWuPEbmZ1SXiP3cwsX9RAY2WGC6g5TuxmVp/kPXYzs1wRPivGzCx3nNjNzPLE57GbmeWNE7uZWa5IoqHZZ8WYmeWHu2LMzPLHid3MLGcaGlTtEDKR2c2sJe0taVnB9Lykj2W1PTOzckhCDaVNJbY3S9KDklZJmtfD8n8uyIfLJbVL2ild9qik+9JlS/v73LK8mfWDwEwASY3A48CCrLZnZlauxsbK7NumOe4i4ARgLbBE0sKIuL+zTkR8FfhqWv9twMcj4pmCZo6NiI2ViCezPfYixwEPR8RfB2h7Zma9E5XcYz8MWBURqyOiFbgSOKWX+qcDV1TgWfRooBL7HDJ8EmZm5UpGd6xYYp8CPFZQXpvO23q70ghgFnBNwewAbpJ0p6S52/eMXpX5wVNJQ4CTgQu2sXwuMBdg10njWbPkVgC+dcmvAPjYldcBsPzkRV3rfO/AzQDc8Xe7ArD47ecA8MYf/xcAHz/obABG7HIMAO2RrDfvlysAOGP8CAA+vXgVAP952r5dbW984HYA9vjXUwF4+TN/BKDhiPOTeBvuAuCv7AjA0FE7JbE8/jwAw8ftAsAfVye/sEZO2LWr7TsfSeaNSbf/7JN/A2D0uOEAbFizCYCp08YAsGbl2qS84+4A3L45WX/y2GEAtL6wqavtSaOTeW0vJW2OGZa8tR1tW5Ly0KTc3tYKwA5DknK0t3cvd7R3tTmsqaHbvCGN6lZuLvqH7yx3Lm8s+jw09vAB2aqOus8oXqW4hZ4+c+qjjb7aLEWD+nfQrZ+rZ0YR1Q5hAKmc93F8Ud93S0S0dGtsa9t6Md8G/LGoG+aoiFgnaSJws6QHImJxqcEVG4izYk4E7oqIJ3tamL44LQCv22fPynQwmZn1Je2KKdHGiDi0l+VrgWkF5anAum3U3aoHIyLWpX83SFpA0rWz3Yl9ILpiMu1LMjPbXhXsilkCzJC0e9pLMQdYuNX2pDHAm4BfFMwbKWlU52PgLcDy/jyvTPfY076kE4BzstyOmVm5JGhsqkyfWES0SToPuBFoBC6NiBWSPpgun59WfTtwU0S8ULD6JGBB2oXYBPw0Im7oTzyZJvaIeBEYl+U2zMy2V/HxmP6IiEXAoqJ584vKlwGXFc1bDRxUsUDwladmVqck5fbKUyd2M6tbZRw8HVSc2M2sbjmxm5nlifp/PUKtcmI3s7okREPTQF18P7Cc2M2sPim/w/Y6sZtZ3ark6Y61xIndzOpSMghYtaPIhhO7mdUnd8WYmeWNaKjQjTZqjRO7mdUleY/dzCx/fIGSmVmOSD3f/CUPnNjNrG45sZuZ5YiQE7uZWZ5IMMRDCpiZ5YcETd5jNzPLD+E+djOzfFF++9gz7WCSNFbS1ZIekLRS0huy3J6ZWamSPfaGkqaS2pNmSXpQ0ipJ83pYfoykTZKWpdO/lbpuubLeY/8WcENEvEPSEGBExtszMytZpfbYJTUCFwEnAGuBJZIWRsT9RVV/HxEnbee6JcsssUsaDbwReC9ARLQCrVltz8ysHA1SJc+KOQxYFRGrASRdCZwClJKc+7Nuj7LcY98DeAr4kaSDgDuB8yPihcJKkuYCcwFG0cjV1yW/Qu44+JcAfPbF6wF4fO7ruta5+o3nAHDafb8C4CM7HwvAUx37ArBD+mad/9O7kzb23BGA992yDICL5yY9Qk/dcjsAe373A11tv3L2NcmDo+cA0NC0BIAHXh4JwPAdJwHw20eeSbY1aToAN6/cAMCYXXYH4M5VGwEYN3lUV9tPP7EZgLETkrbWPvQ0AHvPGAfAI8tWJy/chNcAcNumpwDYbXzyQ6f1hU0ATB4zDIC2l199KcePaAagvfVlAHYclpbbku/SMcOSt7pjS1IeNbQxKafLhzcnr1l0tHe1OaRR3eY1F+3ddJ5R0Lm8+AyD4r2hxh52jorHwy7egSpeZavlPYyn3ddOWF/7aMW3S9ue26cVr1Krw34rotohVFVj6W/MeElLC8otEdFSUJ4CPFZQXgsc3kM7b5B0D7AO+FRErChj3ZJlmdibgEOAj0TEHZK+BcwDPltYKX1xWgB21tD6/i8zswFT5pACGyPi0N6a62FecT67C9gtIv4maTZwHTCjxHXLkuXB07XA2oi4Iy1fTZLozcxqQmODSppKsBaYVlCeSrJX3iUino+Iv6WPFwHNksaXsm65MkvsEfEE8JikvdNZx9GPPiMzs0rqvECplKkES4AZknZPTxSZAyzsvj3trLTvUNJhJPn36VLWLVfWZ8V8BPhJGuxq4H0Zb8/MrCSicgdPI6JN0nnAjUAjcGlErJD0wXT5fOAdwIcktQEvAXMiIoAe1+1PPJkm9ohYBvTWL2VmVhWVHrY37V5ZVDRvfsHj7wLfLXXd/vCVp2ZWlzykgJlZ3vhGG2Zm+eLx2M3McsiJ3cwsRxp8ow0zs5xxH7uZWb4IlTNWzKDixG5mdWt7BngbDJzYzawuiZ5HHM0DJ3Yzq0+CBvexm5nlh4DmEm97N9g4sZtZXXJXjJlZ3kjuijEzyxPhs2LMzHLHXTFmZjkiQXOjD56ameWGu2LMzHIor10xmf4OkfSopPskLZO0NMttmZmVQ4gGlTaV1J40S9KDklZJmtfD8ndLujedbpN0UMGyiubKgdhjPzYiNg7AdszMSlfB0R0lNQIXAScAa4ElkhZGxP0F1R4B3hQRz0o6EWgBDi9YXrFc6a4YM6tLSR97xZo7DFgVEasBJF0JnAJ0JfaIuK2g/u3A1IptvUjWiT2AmyQF8P2IaCmuIGkuMBdgYlMzEz7xjwDM+8UFAHzupC8CcM66e7rWuXX+AQDcdOtzABw+dhgAF37/DgCuPmUvAC6++RYA/u5LSZsbv7AEgMnfTNpu+9WXAHhi9zd1td088jdJ249uBmDULnsCcNU96wAYOz3Z9rV3PQ7AuN12BeDeB58CYOK0MQBsWPs8AK/Zd0JX2/fe/ggAbzhkCgAP3nYfAPtM3j/Z5vNJGzMm7QDAlhc2ATBldPL82l5+IdnGyKFJufWlrrbHjxgCQPuWVgB2Gt4MQEdaHjU0eaujox2AYekNBjrLQ9LOxs5y4byuckP3Os1Fn4riEwyatlq+9aeouI9zq3LROir6WdzTB7N4Vl+f3UocQKvVY3CK6LVcz8ocUmB8URdJS1E+mwI8VlBeS/e98WIfAH5dUO4zV5Yj68R+VESskzQRuFnSAxGxuLBC+gRaAPYaNsL/dWY2MLT1zkgvNkbEob23tpUe85mkY0kS+9EFs/vMleXI9OBpRKxL/24AFpD8XDEzq7rO0x0rdPB0LTCtoDwVWLfVNqUDgUuAUyLi6c75lc6VmSV2SSMljep8DLwFWJ7V9szMypPcQamUqQRLgBmSdpc0BJgDLOy2NWlX4FrgzIj4S8H8iufKLLtiJgEL0j7RJuCnEXFDhtszMytZJS9Qiog2SecBNwKNwKURsULSB9Pl84F/A8YBF6d5sS3t3ql4rswssadHhw/qs6KZWRUkQwpU7qh3RCwCFhXNm1/w+Gzg7B7Wq3iu9OmOZla3avVspv5yYjezutXQ58mwg5MTu5nVJeE9djOz3MnpDZSc2M2sTim/e+wlnccu6TRJD0naJOl5SZslPZ91cGZmWVFlz2OvKaXusX8FeFtErMwyGDOzgVTvXTFPOqmbWd7kNK+XnNiXSvoZcB3wSufMiLg2k6jMzDLmW+PBaOBFkjEMOgXJuAdmZoNSTvN6aYk9It6XdSBmZgMt0+Ftq6jUs2KmSlogaYOkJyVdIymzu3+YmWVN6a3xSpkGm1K/sH5EMgTlLiR3CvllOs/MbNCSSpsGm1IT+4SI+FFEtKXTZcCEvlYyM6tVIkmApUyDTakxb5R0hqTGdDoDeLrPtczMapikkqbBptTE/n7gH4AngPXAO9J5ZmaDk5ILlEqZBptSz4pZA5yccSxmZgNGQAXvs1FTek3skj4dEV+R9B16uON2RHw0s8jMzDI2GLtZStFXV0znMAJLgTt7mPqU9snfLen67Y7SzKzCkitPK9cVI2mWpAclrZI0r4flkvTtdPm9kg4pdd1y9brHHhG/TB++GBE/LwrynSVu43ySL4jR5YdnZpadSu2vS2oELgJOANYCSyQtjIj7C6qdCMxIp8OB7wGHl7huWUo9eHpBifO6SS9i+nvgknKCMjPLnmhQaVMJDgNWRcTqiGgFrgROKapzCvA/kbgdGCtpconrlqWvPvYTgdnAFEnfLlg0Gmgrof1vAp8GRvWyjbnAXICJTc0lNGlmVgHlXXw0XtLSgnJLRLQUlKcAjxWU15LsldNHnSklrluWvs6KWUfSv34y3fvUNwMf721FSScBGyLiTknHbKte+uK0ABww85C45NcPA7DlPUn30+EjhwDw1s/9pmudq9+5LwBvuuQaAL5zydkAnPeFpOdo/0XJd9CLs78EwDPHJl1Wzd/4LACLnk2+Z8bsmrQz/45XX9MJ+xyRzFu8GoCd994PgBv/nNSZ8prkuqxHHtwIwGv2Tcr33v4IALNn7w/AdXcmv6JeP3vvrrb/9MvfAXDg1CMBuHLTU0kb40cC0Lr5WQCmjRmelF9M7mUyZfQwANpeeQmASelr0rGltavtnUYkX4odbcm8UUMbu5WHNyU/zqKjvVu507D09IDO5QBDiuY1FZ1C0FTU+dhcVC4+46CnMxCKL9cuPphV3L9Z3EYpn8viPa5yR/TrqXotHHNTRK9l650iUMH/ex82RsShvTXXw7ziN2RbdUpZtyx99bHfA9wj6ScRUcoeeqGjgJMlzQaGAaMl/W9EnLGdsZqZVZSio1JNrQWmFZSnkuwYl1JnSAnrlqXXPnZJV6UP706P4nZO90m6t7d1I+KCiJgaEdOBOcD/c1I3s9oREB2lTX1bAsyQtLukISQ5b2FRnYXAe9KzY44ANkXE+hLXLUtfXTHnp39P6s9GzMxqUoW6ryKiTdJ5wI1AI3BpRKyQ9MF0+XxgEckxy1Uk97d4X2/r9ieevrpi1qcPNwIvRUSHpL2AfYBfl7qRiPgt8NvtjNHMrPIiSt0bL7G5WESSvAvnzS94HMC5pa7bH6We7rgYGCZpCnALyTfNZZUKwsysGhQdJU2DTamJXRHxInAa8J2IeDuwX3ZhmZllLaCjrbRpkCn1nqeS9Abg3cAHylzXzKz2BBXtiqklpSbnj5FcabogPSCwB3BrdmGZmWUtoKOOE3tE/A74naRRknaIiNWAR3Y0s0FtMPafl6LUm1kfIOluYDlwv6Q7Je2fbWhmZhmr3HnsNaXUrpjvA5+IiFsB0iECfgAcmVFcZmbZioDShxQYVEpN7CM7kzok56VLGplRTGZmAyKvXTGlJvbVkj4L/DgtnwE8kk1IZmYDobIXKNWScm5mPQG4Np3Gk14Oa2Y2aNVjH7ukYcAHgdcA9wGfjIgtAxGYmVmmKjykQC3pqyvmcmAL8HuS2zrtS3JOu5nZoCbqt499v4g4AEDSD4E/Zx+SmdlACGivz7Niurpd0qElMw7HzGyA1PGQAgdJej59LGB4WhbJKJSjM43OzCxDddkVExGNAxWImdnAqt+Dp2Zm+eXEbmaWIzkeUqDUC5TKJmmYpD9LukfSCkn/kdW2zMzKF0TblpKm/pC0k6SbJT2U/t2xhzrTJN0qaWWaL88vWPbvkh6XtCydZve1zcwSO/AK8OaIOAiYCcxK78xtZlZ9QbLHXsrUP/OAWyJiBsmtRef1UKeN5ALQfYEjgHMlFd6l7hsRMTOd+rw3amaJPRJ/S4vN6VSZW4KbmfVTEER7e0lTP51CcrEn6d9Tt4olYn1E3JU+3gysBKZs7waz3GNHUqOkZcAG4OaIuKOHOnMlLZW09JmnN2YZjpnZq4LkDkqlTP0zKSLWQ5LAgYm9VZY0HTgYKMyX50m6V9KlPXXlFMv04GlEtAMzJY0FFkh6bUQsL6rTArQANI2ZEl+59L0ATPjqxQC0LPsZAOed+q2udXb53TUAtL41+UVz24FJd9SoyS0A/N/7kx8GOx90LADnX7cCgOmHvRmAL6XlPV9/MAALbl7V1fbeh0wF4P6ljwFw3PF7AfDrBbcD8P73HgPA9+dfD8C5/3AAAH+4+gYAjtzjjQD89Ol1AMycMqar7Vc2JV9ce49PRjx+ZfOzAOyx0wgAtryU/MDZdcwwANpfeQmASTsMScqtSXnH4cnb1tHW2tX2Ds3Jd3SkPxtHFJWHN3UvD21St/KQxq0vPiueN6She7m5aLegqWh5Yx9lgOJZxWEUr9FQdJFccXlb87q1WbyNPsoDRRFlla2/yjp4Ol7S0oJyS5q7AJD0G2DnHta7sJyIJO0AXAN8LCI6ryH6HvD5JGA+D3yNZGDGbRqQs2Ii4jlJvwVmkdyFycysuiLKOTC6MSIO3XZTcfy2lkl6UtLkiFgvaTJJD0ZP9ZpJkvpPIuLagrafLKjzA+D6voLN8qyYCemeOpKGA8cDD2S1PTOz8gTR0V7S1E8LgbPSx2cBvyiuoGS8lh8CKyPi60XLJhcU304JO8dZ7rFPBi6X1EjyBXJVRPT5TWNmNiA6z4rJ3peBqyR9AFgDvBNA0i7AJRExGzgKOBO4Lz0uCfAv6RkwX5E0M434UeCcvjaYWWKPiHtJDgCYmdWgqMSB0b63EvE0cFwP89cBs9PHf2DrQ0qd9c4sd5u+8tTM6lNQiVMZa5ITu5nVqfwOKeDEbmb1qbyzYgYVJ3Yzq1PeYzczy5eBOytmwDmxm1ldCoIYgLNiqsGJ3czqk/fYzcxyJoLY0tp3vUHIid3M6tTAXKBUDU7sZla/3BVjZpYjEZUY4KsmObGbWd3yWTFmZnkSQbQ7sZuZ5UZE0LGlrdphZMKJ3czqU+A9djOzvHFiNzPLkYigw+Oxm5nlS17PisnyZtbTJN0qaaWkFZLOz2pbZmZlS8+KKWUabLLcY28DPhkRd0kaBdwp6eaIuD/DbZqZlWSgzoqRtBPwM2A6yc2o/yEinu2h3qPAZqAdaIuIQ8tZv1Bme+wRsT4i7kofbwZWAlOy2p6ZWbk62jtKmvppHnBLRMwAbknL23JsRMzsTOrbsT6QYWIvJGk6cDBwRw/L5kpaKmlptL4wEOGYmXWd7jgAXTGnAJenjy8HTs16/cwPnkraAbgG+FhEPF+8PCJagBaAIeOmx7nNJwOw+9GPA/CmK58B4KBT53St8+Yv3ArAG05/JwDnfG0xALP/8a0AXHzJbwF4zxlHA3BJy68AuOBTpwHwhS/+BIBvfPF9AHz0n7/X1fYX3v+RZN0rFwBw+mfeDMAV31oJwEn7vQuArz3xKABv3G0nAF569kkAXrfLaABe2ZzEve/4kV1tt76wKXluY4cB0N76EgC7jBrSrTxuePK2dLQlQ4qOHdaYvFbpuBajhzR0KwOMHtq9zg7N3b+zRzarW3lYY/fy0Katv+OHFNVpLio3NXQvFy2meavlRRV6mNdQVG4saqO4iR6aLKlOfymiomWrgvKuPB0vaWlBuSXNXaWYFBHrk03GekkTtxURcJOkAL5f0H6p63fJNLFLaiZJ6j+JiGuz3JaZWTmCss6K2VjUPdKNpN8AO/ew6MIyQjoqItaliftmSQ9ExOIy1u+SWWKXJOCHwMqI+HpW2zEz2y4RdLRW5uBpRBy/rWWSnpQ0Od3bngxs2EYb69K/GyQtAA4DFgMlrV8oyz72o4AzgTdLWpZOszPcnplZ6QI6OjpKmvppIXBW+vgs4BfFFSSNTM8eRNJI4C3A8lLXL5bZHntE/AHIoHfTzKz/ggEb3fHLwFWSPgCsAd4JIGkX4JKImA1MAhYkHR00AT+NiBt6W783vvLUzOpTQAzAkAIR8TRwXA/z1wGz08ergYPKWb83TuxmVqcit0MKOLGbWX3ysL1mZvkSEbRX6KyYWuPEbmZ1yl0xZmb54q4YM7OcCYj2fA7t4MRuZnUpiEqM3FiTnNjNrD4FRIf32M3MciMC2lt9z1Mzs/yIcB+7mVnedDixm5nliE93NDPLlwA6fPDUzCxHInzw1MwsT8IXKJmZ5YwTu5lZ3uT3ytPM7nkq6VJJGyQt77u2mdkAS688LWUabLK8mfVlwKwM2zcz225Bch57KVN/SNpJ0s2SHkr/7thDnb0lLSuYnpf0sXTZv0t6vGDZ7L62mVlij4jFwDNZtW9m1i8RdLS2lzT10zzgloiYAdySlotCiQcjYmZEzAReB7wILCio8o3O5RGxqK8NZrnHbmZWsyIGZo8dOAW4PH18OXBqH/WPAx6OiL9u7warfvBU0lxgLsC0adP42dcuBuD52y4CYPSR53YrF85bUlTnB99Iy2kbn3vzmQB87V9XAvDhQ3cBYN6TjwLwrv3GA/BPzz7R1faJe44F4JXNyY+No6fuAEDby38DYObE4QC0t74EwF47DgGgo60VgD3GNHcrTxv16kscHck3/+SRTd3KE4c3dntNxg3r/n07dkj38ughW38fj2xSt/KIovKwxt7LQ3v4im9W7+WmPsoNRK/lUuooyitvzzrbsw3LhzLuoDRe0tKCcktEtJS47qSIWA8QEeslTeyj/hzgiqJ550l6D7AU+GREPNtbA1VP7OmL0wLwukMOiSf8GTKzgRBl7Y1vjIhDt7VQ0m+AnXtYdGE5IUkaApwMXFAw+3vA50kOC3we+Brw/t7aqXpiNzOrigqexx4Rx29rmaQnJU1O99YnAxt6aepE4K6IeLKg7a7Hkn4AXN9XPFme7ngF8Cdgb0lrJX0gq22ZmZUrSAYBK2Xqp4XAWenjs4Bf9FL3dIq6YdIvg05vB/o8hTyzPfaIOD2rts3M+i2C9tYBuUDpy8BV6c7tGuCdAJJ2AS6JiNlpeQRwAnBO0fpfkTST5Lvo0R6Wb8VdMWZWlyKgYwAOjEfE0yRnuhTPXwfMLii/CIzrod6Z5W7Tid3M6ukSuvMAAAa5SURBVFZ7Ts94cmI3s7oUQE7HAHNiN7P65T12M7Mc6QhoHYQDfJXCid3M6pa7YszMciQId8WYmeWJD56ameWQE7uZWY5E+KwYM7NcCXxWjJlZrriP3cwsh9wVY2aWI0kfe7WjyIYTu5nVLe+xm5nlSAADMhp7FTixm1ldCsJnxZiZ5UlyVowTu5lZfuT44GlmN7MGkDRL0oOSVkmal+W2zMzK0bnHXso02GS2xy6pEbiI5Oasa4ElkhZGxP1ZbdPMrBx53WPPsivmMGBVRKwGkHQlcArgxG5mVddBfocUUGT0M0PSO4BZEXF2Wj4TODwiziuqNxeYmxZfCyzPJKDKGQ9srHYQJXCcleU4K6u/ce4WERP6E4CkG9I4SrExImb1Z3sDKcs9dvUwb6tvkYhoAVoAJC2NiEMzjKnfBkOM4DgrzXFWVi3EOZgSdbmyPHi6FphWUJ4KrMtwe2ZmRraJfQkwQ9LukoYAc4CFGW7PzMzIsCsmItoknQfcCDQCl0bEij5Wa8kqngoaDDGC46w0x1lZgyXOQSmzg6dmZlYdmV6gZGZmA8+J3cwsZ2oisdfq0AOSpkm6VdJKSSsknZ/O30nSzZIeSv/uWAOxNkq6W9L1NRzjWElXS3ogfU3fUKNxfjx9v5dLukLSsFqIU9KlkjZIWl4wb5txSbog/Uw9KOmtVY7zq+n7fq+kBZLGVjvOPKt6Yi8YeuBEYD/gdEn7VTeqLm3AJyNiX+AI4Nw0tnnALRExA7glLVfb+cDKgnItxvgt4IaI2Ac4iCTemopT0hTgo8ChEfFakgP/c6iNOC8Dis+97jGu9P90DrB/us7F6WetWnHeDLw2Ig4E/gJcUANx5lbVEzsFQw9ERCvQOfRA1UXE+oi4K328mSQRTSGJ7/K02uXAqdWJMCFpKvD3wCUFs2stxtHAG4EfAkREa0Q8R43FmWoChktqAkaQXH9R9TgjYjHwTNHsbcV1CnBlRLwSEY8Aq0g+a1WJMyJuioi2tHg7yXUtVY0zz2ohsU8BHisor03n1RRJ04GDgTuASRGxHpLkD0ysXmQAfBP4NN1vCFNrMe4BPAX8KO0yukTSSGoszoh4HPhvYA2wHtgUETdRY3EW2FZctfy5ej/w6/RxLcc5aNVCYi9p6IFqkrQDcA3wsYh4vtrxFJJ0ErAhIu6sdix9aAIOAb4XEQcDL1Ab3UPdpH3UpwC7A7sAIyWdUd2otktNfq4kXUjSxfmTzlk9VKt6nINdLST2mh56QFIzSVL/SURcm85+UtLkdPlkYEO14gOOAk6W9ChJN9abJf0vtRUjJO/z2oi4Iy1fTZLoay3O44FHIuKpiNgCXAscSe3F2WlbcdXc50rSWcBJwLvj1Qtoai7OPKiFxF6zQw9IEkmf8MqI+HrBooXAWenjs4BfDHRsnSLigoiYGhHTSV67/xcRZ1BDMQJExBPAY5L2TmcdRzKEc03FSdIFc4SkEen7fxzJsZVai7PTtuJaCMyRNFTS7sAM4M9ViA9IznwDPgOcHBEvFiyqqThzIyKqPgGzSY6UPwxcWO14CuI6muRn4b3AsnSaDYwjOQPhofTvTtWONY33GOD69HHNxQjMBJamr+d1wI41Gud/AA+QDCH9Y2BoLcQJXEHS77+FZE/3A73FBVyYfqYeBE6scpyrSPrSOz9H86sdZ54nDylgZpYztdAVY2ZmFeTEbmaWM07sZmY548RuZpYzTuxmZjnjxG4DQlK7pGXpqIn3SPqEpO3+/5P0LwWPpxeOJGhW75zYbaC8FBEzI2J/4ASS6wE+14/2/qXvKmb1yYndBlxEbADmAucp0ZiO170kHa/7HABJx0hanI7ffb+k+ZIaJH2ZZPTFZZI6xxxplPSD9BfBTZKGV+v5mVWbE7tVRUSsJvn/m0hyZeKmiHg98Hrgn9LLyyEZwvWTwAHAnsBpETGPV38BvDutNwO4KP1F8Bzwfwbu2ZjVFid2q6bOkf3eArxH0jKSYZHHkSRqgD9HMlZ/O8ml6kdvo61HImJZ+vhOYHo2IZvVvqZqB2D1SdIeQDvJaIQCPhIRNxbVOYath3Dd1hgYrxQ8bgfcFWN1y3vsNuAkTQDmA9+NZLCiG4EPpUMkI2mv9CYcAIelI382AO8C/pDO39JZ38y68x67DZThaVdLM8mNFn4MdA6FfAlJ18ld6VC5T/HqLd7+BHyZpI99MbAgnd8C3CvpLpLRAc0s5dEdrWalXTGfioiTqh2L2WDirhgzs5zxHruZWc54j93MLGec2M3McsaJ3cwsZ5zYzcxyxondzCxn/j91c0N7DKMBcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_seq = 64 # ?? : seq길이는 8인데 왜 64로 설정\n",
    "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\n",
    "print(pos_encoding.shape) # 크기 출력\n",
    "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d_hidn))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### position embedding 값을 구하는 절차\n",
    "1. 위에서 구해진 position encoding값을 이용해 position embedding 생성\n",
    "    - 학습되는 값이 아니므로 freeze 옵션을 True로 설정\n",
    "2. 입력 inputs과 동일한 크기를 갖는 positions 값 구하기\n",
    "3. input값 중 pad(0)값 찾기\n",
    "4. positions 값 중 pad 부분은 0으로 변경\n",
    "5. positions 값에 해당하는 embedding 값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
       "         [0, 1, 2, 3, 4, 5, 6, 7]]),\n",
       " tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
       "         [1, 2, 3, 4, 5, 6, 7, 8]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 궁금해서 찍어봄\n",
    "tmp = torch.arange(8)\n",
    "tmp, tmp.expand(2,8), tmp.expand(2,8).contiguous() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 궁금해서 찍어봄 \n",
    "inputs.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3159, 3533,  200, 3883, 3688, 3519,    0,    0],\n",
       "         [ 206, 3534,   53, 3759, 3525, 3613, 3688, 3519]]),\n",
       " tensor([[1, 2, 3, 4, 5, 6, 0, 0],\n",
       "         [1, 2, 3, 4, 5, 6, 7, 8]]),\n",
       " torch.Size([2, 8, 128]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding = torch.FloatTensor(pos_encoding)\n",
    "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze = True)\n",
    "\n",
    "# torch.arange(end = 8, device = cpu, dtype = torch.int64)\n",
    "positions = torch.arange(inputs.size(1), device = inputs.device, \n",
    "                         dtype=inputs.dtype).expand(inputs.size(0),\n",
    "                                                    inputs.size(1)).contiguous() + 1\n",
    "# inputs.eq(0) : 값이 0일 경우 True 반환\n",
    "pos_mask = inputs.eq(0)\n",
    "positions.masked_fill_(pos_mask, 0)\n",
    "pos_embs = nn_pos(positions) # Position Embedding\n",
    "\n",
    "inputs, positions, pos_embs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_embs, pos_embs는 (2,8,128)로 동일함\n",
    "- 두 값을 더한게 transformer에 입력할 Input값이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sums = input_embs + pos_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scaled Dot Product Attention\n",
    "- [상세 계산 내용 참고](https://simpling.tistory.com/3)\n",
    "1. MatMul : Q,K\n",
    "2. Scale\n",
    "3. Mask(opt.)\n",
    "4. SoftMax\n",
    "5. MatMul : V\n",
    "\n",
    "### 입력값 : Query, Key, Value 그리고 Attention Mask로 구성 됨\n",
    "- 입력값 중 K,V는 같은 값이어야 함\n",
    "    - Q,K,V가 모두 동일할 경우 self attention이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]),\n",
       " tensor([[False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = K = V = inputs_sums # self attention\n",
    "\n",
    "# inputs.eq(0) : pad(0) 부분만 True\n",
    "# .unsqueeze(1) : size[2,8] -> 두 번째 차원에 1인 차원 추가 -> size[2, 1, 8]\n",
    "# .expand(2,8,8)\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "attn_mask.size(), attn_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. MatMul : Q, K\n",
    "### 각 단어 상호간에 가중치를 표현하는 테이블 생성\n",
    "- Query와 Key의 행렬 계산은 Q * K-transpose 수행\n",
    "    - 이는 내적 연산과 같은 과정으로 각 단어에 대한 attention score를 구할 수 있음 \n",
    "- 내적 연산 결과는 -1~1사이의 값을 가짐\n",
    "    - 어떤 쿼리와 키가 중요한 역할을 하고 있다면 attention block은 이들 사이의 내적 값을 키우는 방식으로 학습\n",
    "    - 내적 값이 커지면 해당 쿼리와 키가 벡터 공간상 가까이 있을 가능성이 큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]),\n",
       " tensor([[256.1583, 104.4680,  65.3711,  29.4813,  62.5547,  62.6991,  88.3033,\n",
       "           88.3033],\n",
       "         [104.4680, 195.1366,  82.5526,  25.6877,  53.8009,  73.9358,  90.9810,\n",
       "           90.9810],\n",
       "         [ 65.3711,  82.5526, 182.4587,  34.8768,  60.5217,  84.4586,  75.8325,\n",
       "           75.8325],\n",
       "         [ 29.4813,  25.6877,  34.8768, 166.8790,  44.9379,  44.0540,  -2.1669,\n",
       "           -2.1669],\n",
       "         [ 62.5547,  53.8009,  60.5217,  44.9379, 177.5070,  78.3394,  27.0040,\n",
       "           27.0040],\n",
       "         [ 62.6991,  73.9358,  84.4586,  44.0540,  78.3394, 197.2271,  65.5387,\n",
       "           65.5387],\n",
       "         [ 88.3033,  90.9810,  75.8324,  -2.1669,  27.0040,  65.5387, 216.6062,\n",
       "          216.6062],\n",
       "         [ 88.3033,  90.9810,  75.8324,  -2.1669,  27.0040,  65.5387, 216.6062,\n",
       "          216.6062]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-1,-2))\n",
    "scores.size(), scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Scale\n",
    "- 행렬 연산(내적) 후 벡터의 차원이 커지면 학습이 잘 안될 수 있으므로 스케일링 과정 거침\n",
    "    - k-dimension에 루트를 취한 값으로 나누어 줌\n",
    "- 가중치 편차가 scale 전과 비교하여 줄어듬을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]),\n",
       " tensor([[32.0198, 13.0585,  8.1714,  3.6852,  7.8193,  7.8374, 11.0379, 11.0379],\n",
       "         [13.0585, 24.3921, 10.3191,  3.2110,  6.7251,  9.2420, 11.3726, 11.3726],\n",
       "         [ 8.1714, 10.3191, 22.8073,  4.3596,  7.5652, 10.5573,  9.4791,  9.4791],\n",
       "         [ 3.6852,  3.2110,  4.3596, 20.8599,  5.6172,  5.5068, -0.2709, -0.2709],\n",
       "         [ 7.8193,  6.7251,  7.5652,  5.6172, 22.1884,  9.7924,  3.3755,  3.3755],\n",
       "         [ 7.8374,  9.2420, 10.5573,  5.5068,  9.7924, 24.6534,  8.1923,  8.1923],\n",
       "         [11.0379, 11.3726,  9.4791, -0.2709,  3.3755,  8.1923, 27.0758, 27.0758],\n",
       "         [11.0379, 11.3726,  9.4791, -0.2709,  3.3755,  8.1923, 27.0758, 27.0758]],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_head = 64\n",
    "scores = scores.mul_(1/d_head ** 0.5)\n",
    "scores.size(), scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Mask (Opt.)\n",
    "- 자신보다 뒤에 있는 단어를 참고하지 않게 해주기 위해 mask해줌 \n",
    "    - 전체 문장을 한 번에 행렬 형태로 입력하는 구조임\n",
    "    - 따라서 mask를 하지않을 경우 자신보다 뒤에 있는 단어를 참고해서 단어를 예측하는 상황이 생길 수 있음\n",
    "- Qeury 단어 뒤에 나오는 Key 단어들에 대해서 masking 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]),\n",
       " tensor([[ 3.2020e+01,  1.3058e+01,  8.1714e+00,  3.6852e+00,  7.8193e+00,\n",
       "           7.8374e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.3058e+01,  2.4392e+01,  1.0319e+01,  3.2110e+00,  6.7251e+00,\n",
       "           9.2420e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 8.1714e+00,  1.0319e+01,  2.2807e+01,  4.3596e+00,  7.5652e+00,\n",
       "           1.0557e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 3.6852e+00,  3.2110e+00,  4.3596e+00,  2.0860e+01,  5.6172e+00,\n",
       "           5.5068e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.8193e+00,  6.7251e+00,  7.5652e+00,  5.6172e+00,  2.2188e+01,\n",
       "           9.7924e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.8374e+00,  9.2420e+00,  1.0557e+01,  5.5068e+00,  9.7924e+00,\n",
       "           2.4653e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.1038e+01,  1.1373e+01,  9.4791e+00, -2.7086e-01,  3.3755e+00,\n",
       "           8.1923e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.1038e+01,  1.1373e+01,  9.4791e+00, -2.7086e-01,  3.3755e+00,\n",
       "           8.1923e+00, -1.0000e+09, -1.0000e+09]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill_(attn_mask, -1e9)\n",
    "scores.size(), scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. Softmax\n",
    "- 0~1사이의 확률값으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]),\n",
       " tensor([[1.0000e+00, 5.8239e-09, 4.3931e-11, 4.9480e-13, 3.0894e-11, 3.1457e-11,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.1964e-05, 9.9999e-01, 7.7298e-07, 6.3263e-10, 2.1249e-08, 2.6326e-07,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.4023e-07, 3.7706e-06, 9.9999e-01, 9.7329e-09, 2.4012e-07, 4.7850e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.4763e-08, 2.1636e-08, 6.8237e-08, 1.0000e+00, 2.3999e-07, 2.1489e-07,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [5.7491e-07, 1.9248e-07, 4.4590e-07, 6.3568e-08, 9.9999e-01, 4.1353e-06,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.9762e-08, 2.0272e-07, 7.5537e-07, 4.8386e-09, 3.5153e-07, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7502e-01, 5.2411e-01, 7.8896e-02, 4.5996e-06, 1.7633e-04, 2.1789e-02,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7502e-01, 5.2411e-01, 7.8896e-02, 4.5996e-06, 1.7633e-04, 2.1789e-02,\n",
       "          0.0000e+00, 0.0000e+00]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "attn_prob.size(), attn_prob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-5. MatMul atten_prov, V\n",
    "- value와 행렬곱을 해줘서 각 context vector 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(attn_prob, V)\n",
    "context.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Procudt Attention Class\n",
    "- 위 절차를 하나의 클래스 형태로 구성하면 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scale dot product attention\"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q,K.transpose(-1,-2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class를 이용하여 context vector, attention prob 구하기\n",
    "attention = ScaledDotProductAttention(64)\n",
    "Q = K = V = inputs_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "context, attn_prob = attention.forward(Q,K,V,attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = K = V = inputs_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Q, K, V\n",
    "- Q, K, V를 여러개의head로 나누는 과정  \n",
    " **nn.Linear(in_features, out_features)**\n",
    "- in_features : size of each input sample\n",
    "- out_features : size of each output sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_V = nn.Linear(d_hidn, n_head * d_head)\n",
    "\n",
    "q_s = W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "k_s = W_K(K).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "v_s = W_V(V).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "print(q_s.size(), k_s.size(), v_s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# attention mask 도 multi head로 변경\n",
    "print(attn_mask.size())\n",
    "attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
    "print(attn_mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "- scaled Dot product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 8, 64])\n",
      "torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
    "context, attn_prob = scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "print(context.size())\n",
    "print(attn_prob.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi head를 한 개로 합침\n",
    "context = context.transpose(1,2).contiguous().view(batch_size, -1, n_head * d_head)\n",
    "context.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n",
    "- 입력 Q와 동일한 shape을 가진 multi head attention이 구해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(n_head * d_head, d_hidn)\n",
    "output = linear(context)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head Attention Class\n",
    "- 위 절차를 하나의 클래스 형태로 구성하면 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_hidn, n_head, d_head):\n",
    "        super().__init__()\n",
    "        self.d_hidn = d_hidn\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.W_V = nn.Linear(d_hidn, n_head * d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
    "        self.linear = nn.Linear(n_head * d_head, d_hidn)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Masked Multi-Head Attention\n",
    "- Multi-Head Attention과 attention mask를 제외한 부분은 모두 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" attention decoder mask \"\"\"\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "Q = K = V =inputs_sums\n",
    "\n",
    "attn_pad_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "print(attn_pad_mask[1])\n",
    "attn_dec_mask = get_attn_decoder_mask(inputs)\n",
    "print(attn_dec_mask[1])\n",
    "attn_mask = torch.gt((attn_pad_mask + attn_dec_mask), 0)\n",
    "print(attn_mask[1])\n",
    "\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "- 위와 동일하므로 클래스 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128]) torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "attention = MultiHeadAttention(d_hidn, n_head, d_head)\n",
    "output, attn_prob = attention(Q, K, V, attn_mask)\n",
    "print(output.size(), attn_prob.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FeedForward\n",
    "$FFN(x)=max(0,xW_1+b_1)W_2+b_2$\n",
    "- The feed-forward layer is weights that is trained during training and the exact same matrix is applied to each respective token position.\n",
    "- Since it is applied without any communcation with or inference by other token positions it is a highly parallelizable part of the model.\n",
    "- The role and purpose is to process the output from one attention layer in a way to better fit the input for the next attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 : Linear\n",
    "$f_1 = xW_1+b_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv1d(in_channels=d_hidn, out_channels=d_hidn * 4, kernel_size=1)\n",
    "# (bs, d_hidn * 4, n_seq)\n",
    "ff_1 = conv1(output.transpose(1, 2))\n",
    "print(ff_1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f2\n",
    "$f_2 = max(0,f_1)$\n",
    "### Activation\n",
    "- relu or gelu\n",
    "- 논문에는 relu를 사용했지만 이후 gelu를 사용할 때 더 성능이 좋다는 것이 발견됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active = F.relu\n",
    "active = F.gelu\n",
    "ff_2 = active(ff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f3(Linear)\n",
    "$f_3 = f_2w_2 + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv1d(in_channels=d_hidn * 4, out_channels=d_hidn, kernel_size=1)\n",
    "ff_3 = conv2(ff_2).transpose(1, 2)\n",
    "print(ff_3.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoswiseFeedForwardNet\n",
    "- Multi-Head Attention과 attention mask를 제외한 부분은 모두 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_hidn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_hidn * 4, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_hidn * 4, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
